{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description: Next Word Prediction Using LSTM\n",
    "#### Project Overview:\n",
    "\n",
    "This project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n",
    "\n",
    "1- Data Collection: We use the text of Shakespeare's \"Hamlet\" as our dataset. This rich, complex text provides a good challenge for our model.\n",
    "\n",
    "2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n",
    "\n",
    "3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5- Model Evaluation: The model is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n",
    "\n",
    "6- Deployment: A Streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading gutenberg: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    }
   ],
   "source": [
    "## Data Collection\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import  pandas as pd\n",
    "\n",
    "## load the dataset\n",
    "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
    "## save to a file\n",
    "with open('hamlet.txt','w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Preprocessing\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # For converting text to numbers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For making sequences uniform length\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "\n",
    "# Load the dataset\n",
    "# Opens 'hamlet.txt' file and reads all text, converting to lowercase\n",
    "# This helps standardize the text and reduce vocabulary size\n",
    "with open('hamlet.txt', 'r') as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "# Tokenization Process\n",
    "# Create a tokenizer object that will convert words to numerical indices\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit_on_texts learns the vocabulary from the text\n",
    "# It creates a word-to-index mapping dictionary where:\n",
    "# - Each unique word gets assigned a unique integer index\n",
    "# - More frequent words typically get smaller indices\n",
    "# - The text is passed as a list containing one string\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# Calculate total number of unique words\n",
    "# Add 1 to account for the 0 index which is reserved for padding\n",
    "# tokenizer.word_index is a dictionary where:\n",
    "#   - keys are words from the text\n",
    "#   - values are their assigned integer indices\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Print the total number of unique words in the vocabulary\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1, 'and': 2, 'to': 3, 'of': 4}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Using dict items and list slicing\n",
    "first_4 = dict(list(tokenizer.word_index.items())[:4])\n",
    "first_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input sequences for training\n",
    "# These sequences will be used to predict the next word\n",
    "input_sequences = []\n",
    "\n",
    "# Split the text into lines and process each line separately\n",
    "for line in text.split('\\n'):\n",
    "    # Convert each line of text to sequences of integers\n",
    "    # texts_to_sequences returns a list of lists, so we take [0] to get the first (and only) sequence\n",
    "    # Example: \"hello world\" might become [45, 67]\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    # Create n-gram sequences\n",
    "    # For each position i, create a sequence from start up to position i+1\n",
    "    # Example with \"hello world\":\n",
    "    # First iteration:  [45]         -> predicts 67\n",
    "    # Second iteration: [45, 67]     -> predicts next word\n",
    "    for i in range(1, len(token_list)):\n",
    "        # Extract the sequence up to position i+1\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        # Add this sequence to our list of input sequences\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 687],\n",
       " [1, 687, 4],\n",
       " [1, 687, 4, 45],\n",
       " [1, 687, 4, 45, 41],\n",
       " [1, 687, 4, 45, 41, 1886]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pad Sequences\n",
    "max_sequence_len=max([len(x) for x in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "        687],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1, 687,\n",
       "          4],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1, 687,   4,\n",
       "         45],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1, 687,   4,  45,\n",
       "         41]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))\n",
    "input_sequences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create predicitors and label\n",
    "import tensorflow as tf\n",
    "x,y=input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target values (y) to one-hot encoded format\n",
    "# to_categorical converts numbers to binary class matrices (one-hot encoding)\n",
    "# Example: if total_words = 5, number 2 becomes [0, 0, 1, 0, 0]\n",
    "\n",
    "# Parameters:\n",
    "# - y: our target values (the next word indices we want to predict)\n",
    "# - num_classes: total number of unique words in our vocabulary (total_words)\n",
    "#   This ensures we have the correct number of columns in our one-hot matrix\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# The resulting 'y' is now a 2D array where:\n",
    "# - Each row represents one training example\n",
    "# - Each row is a binary vector with length total_words\n",
    "# - In each row, only one position has 1 (the target word's index)\n",
    "# - All other positions have 0\n",
    "\n",
    "# Example:\n",
    "# If y was [2, 3] and total_words = 5, y becomes:\n",
    "# [[0, 0, 1, 0, 0],\n",
    "#  [0, 0, 0, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping callback to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create early stopping object that:\n",
    "# - Monitors validation loss \n",
    "# - Stops training if no improvement for 3 epochs\n",
    "# - Keeps the model weights from best epoch\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 13, 150)           113400    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 100)               75600     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1157418 (4.42 MB)\n",
      "Trainable params: 1157418 (4.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define GRU (Gated Recurrent Unit) model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer parameters:\n",
    "# - total_words: Size of the vocabulary\n",
    "# - 100: Dimension of dense embedding (size of vector space for words)\n",
    "# - input_length: Length of input sequences\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "\n",
    "# First GRU layer parameters:\n",
    "# - 150: Number of GRU units (dimensionality of output space)\n",
    "# - return_sequences=True: Return full sequence for stacking layers\n",
    "# GRU is simpler than LSTM and often trains faster while maintaining good performance\n",
    "model.add(GRU(150, return_sequences=True))\n",
    "\n",
    "# Dropout layer to prevent overfitting\n",
    "# - 0.2: 20% of neurons will be randomly disabled during training\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second GRU layer parameters:\n",
    "# - 100: Number of GRU units\n",
    "# - return_sequences=False by default for final sequence processing\n",
    "model.add(GRU(100))\n",
    "\n",
    "# Output layer parameters:\n",
    "# - total_words: Number of units equals vocabulary size for word prediction\n",
    "# - softmax: Activation function to get probability distribution over words\n",
    "model.add(Dense(total_words, activation=\"softmax\"))\n",
    "\n",
    "# Model compilation:\n",
    "# - categorical_crossentropy: Loss function for multi-class classification\n",
    "# - adam: Optimizer with adaptive learning rate\n",
    "# - accuracy: Metric to monitor during training\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Display model architecture summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "644/644 [==============================] - 18s 26ms/step - loss: 7.0577 - accuracy: 0.0308 - val_loss: 6.9601 - val_accuracy: 0.0332\n",
      "Epoch 2/50\n",
      "644/644 [==============================] - 14s 21ms/step - loss: 6.6366 - accuracy: 0.0321 - val_loss: 6.9448 - val_accuracy: 0.0204\n",
      "Epoch 3/50\n",
      "644/644 [==============================] - 13s 21ms/step - loss: 6.4701 - accuracy: 0.0411 - val_loss: 6.9020 - val_accuracy: 0.0443\n",
      "Epoch 4/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 6.2716 - accuracy: 0.0521 - val_loss: 6.8961 - val_accuracy: 0.0480\n",
      "Epoch 5/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 6.0576 - accuracy: 0.0635 - val_loss: 6.8899 - val_accuracy: 0.0618\n",
      "Epoch 6/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.8064 - accuracy: 0.0765 - val_loss: 6.9202 - val_accuracy: 0.0655\n",
      "Epoch 7/50\n",
      "644/644 [==============================] - 13s 21ms/step - loss: 5.5515 - accuracy: 0.0898 - val_loss: 6.9895 - val_accuracy: 0.0641\n",
      "Epoch 8/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.2999 - accuracy: 0.0968 - val_loss: 7.1230 - val_accuracy: 0.0629\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "history=model.fit(x_train,y_train,epochs=50,validation_data=(x_test,y_test),verbose=1,callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word given a text input\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "   # Convert input text to sequence of numbers using tokenizer\n",
    "   token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "   \n",
    "   # If sequence is longer than max length, truncate it\n",
    "   if len(token_list) >= max_sequence_len:\n",
    "       token_list = token_list[-(max_sequence_len-1):]  \n",
    "   \n",
    "   # Pad sequence to ensure consistent length for model input\n",
    "   token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "   \n",
    "   # Get model's prediction (probability distribution over words)\n",
    "   predicted = model.predict(token_list, verbose=0)\n",
    "   \n",
    "   # Get index of word with highest probability\n",
    "   predicted_word_index = np.argmax(predicted, axis=1)\n",
    "   \n",
    "   # Convert predicted index back to word\n",
    "   for word, index in tokenizer.word_index.items():\n",
    "       if index == predicted_word_index:\n",
    "           return word\n",
    "           \n",
    "   # Return None if no matching word found\n",
    "   return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:To be or not to be\n",
      "Next Word PRediction:to\n"
     ]
    }
   ],
   "source": [
    "input_text=\"To be or not to be\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "model.save(\"next_word_gru.h5\")\n",
    "## Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:  Barn. Last night of all,When yond same\n",
      "Next Word PRediction:lord\n"
     ]
    }
   ],
   "source": [
    "input_text=\"  Barn. Last night of all,When yond same\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THis will be  far better model but my system will take alot of time to build this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "# Improved model architecture\n",
    "model = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(\n",
    "        input_dim=total_words,        # Size of vocabulary\n",
    "        output_dim=200,               # Increased from 100: Dimension of word vectors\n",
    "        input_length=max_sequence_len-1  # Length of input sequences\n",
    "    ),\n",
    "    \n",
    "    # Using Bidirectional LSTM for better context understanding\n",
    "    Bidirectional(LSTM(\n",
    "        units=256,                    # Increased number of units\n",
    "        return_sequences=True,        # Return full sequence of outputs\n",
    "        recurrent_dropout=0.1         # Dropout for recurrent connections\n",
    "    )),\n",
    "    \n",
    "    Dropout(0.3),                     # Increased dropout for better regularization\n",
    "    \n",
    "    # Second Bidirectional LSTM\n",
    "    Bidirectional(LSTM(\n",
    "        units=128,                    # Number of LSTM units\n",
    "        recurrent_dropout=0.1\n",
    "    )),\n",
    "    \n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Add an intermediate Dense layer\n",
    "    Dense(512, activation='relu'),    # Additional layer for more complex patterns\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(total_words, activation='softmax')  # Softmax for word prediction\n",
    "])\n",
    "\n",
    "# Improved compilation with learning rate scheduling\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
